{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmwLT5LhhudExmZQXMVIH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewpannn/MNIST/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgRH_VvIcugl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import math\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoidPrime(z):\n",
        "  return np.exp(-z) / np.power(1 + np.exp(-z), 2)\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "#reshapes the array from 60000, 28, 28 to 60000, 784\n",
        "newdata = np.reshape(train_x, (60000, 784))\n",
        "newdata = np.multiply(1/255, newdata)  # scales down the values in the matrix\n",
        "\n",
        "#turns the train_y array into a 60000 * 10 array with zeros filling the unwanted answer and a \"1\" for the right answer\n",
        "correct = np.zeros((60000,10))\n",
        "for x in train_y:\n",
        "  correct[x][train_y[x]] = 1\n",
        "\n",
        "n = 2  # learning rate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class network:\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "    self.layerNum = len(layers)\n",
        "    self.biases = [np.random.randn(x) for x in layers[1:]]  # biases[0] is the biases for the neurons on layer 1\n",
        "    self.weights = [np.random.randn(layers[x], layers[x-1]) for x in range(1, len(layers))]  # makes a matrix for weights. weights[0] is the weights from layer 0 to layer 1\n",
        "\n",
        "thisNetwork = network([784,10,10])"
      ],
      "metadata": {
        "id": "prIJvnN9c58C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward prop\n",
        "\n",
        "# creating the input layer\n",
        "activation = [np.zeros(x) for x in thisNetwork.layers]\n",
        "z = [np.zeros(x) for x in thisNetwork.layers]  # z is the matrix for the weighted inputs\n",
        "activation[0] = newdata[0]\n",
        "\n",
        "\n",
        "# forward prop for all layers except for input layer\n",
        "def forwardProp():\n",
        "  for x in range(0, thisNetwork.layerNum - 1):\n",
        "    for y in range(10):  #range(number of elements in thisNetwork.layers[x])\n",
        "      z[x+1][y] = np.dot(activation[x], thisNetwork.weights[x][y]) + thisNetwork.biases[x][y]\n",
        "      activation[x+1][y] = sigmoid(z[x+1][y])  #propagates forward the weighted input\n",
        "forwardProp()"
      ],
      "metadata": {
        "id": "2y_XWnBxp5pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cost function - compares result with intended result. Not actually necessary\n",
        "def returnCost(result, indexNo):\n",
        "    cost = np.subtract(result, correct[indexNo])\n",
        "    cost = np.power(cost, 2)\n",
        "    cost = 0.5*np.sum(cost)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "tJWM5TdkEfgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#backprop\n",
        "error = [np.zeros(x) for x in thisNetwork.layers[1:]]  # repesents the error of each particular neuron. error[0] is error of 2nd layer\n",
        "#calculate error in final layer\n",
        "def returnFinalError(result, z, correct):\n",
        "    finGrad = np.subtract(result, correct)  # finds the rate of change of the cost with respect to final activation\n",
        "    finError = np.multiply(finGrad, sigmoidPrime(z))\n",
        "    return finError\n",
        "#error[1] = returnFinalError(activation[2], z[2], correct[0])"
      ],
      "metadata": {
        "id": "8VIS5MLBlpEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate error in 2nd layer\n",
        "def returnLayerError(error, z):\n",
        "    newWeights = np.transpose(thisNetwork.weights[1])\n",
        "    for x in range(10):\n",
        "      weightedError = np.dot(error, newWeights[x])  # multiplying error by the transposed weight matrix\n",
        "    newError = np.multiply(weightedError, sigmoidPrime(z))\n",
        "    return newError\n",
        "\n",
        "#error[0] = returnLayerError(error[1], z[1])"
      ],
      "metadata": {
        "id": "nQXFtW23gF3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gradient descent \n",
        "print(returnCost(activation[2],0))\n",
        "#update biases\n",
        "def updateBiases(errorB):\n",
        "    errorB = np.multiply(1/n, errorB)\n",
        "    thisNetwork.biases = np.subtract(thisNetwork.biases, error)  # subtracts the error of each neuron from the bias matrix, elementwise\n",
        "updateBiases(error)"
      ],
      "metadata": {
        "id": "sy8nFKVkPRbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd29b921-edb1-4785-d8ec-c916dc21efb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2964728024384897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#update weights  \n",
        "def updateWeights():\n",
        "    weightGrad = [np.zeros(w.shape) for w in thisNetwork.weights]\n",
        "    for x in range(0, 1):\n",
        "      for y in range(0, 10):\n",
        "        weightGrad[x][y] = np.multiply(activation[x], error[x][y]) #  applies the transposed activation matrix to the error\n",
        "\n",
        "      thisNetwork.weights[x] = np.subtract(thisNetwork.weights[x], 0.1 * weightGrad[x])  \n",
        "\n",
        "updateWeights()"
      ],
      "metadata": {
        "id": "WtNwRghTk0h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(0, thisNetwork.layerNum - 1):\n",
        "  for y in range(10):  #range(number of elements in thisNetwork.layers[x])\n",
        "    z[x+1][y] = np.dot(activation[x], thisNetwork.weights[x][y]) + thisNetwork.biases[x][y]\n",
        "    activation[x+1][y] = sigmoid(z[x+1][y])  #propagates forward the weighted input\n",
        "\n",
        "print(returnCost(activation[2], 0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XygX1SioK-JX",
        "outputId": "777ad28a-c419-4fd5-f3e2-708735c1c99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2964728024384897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "thisNetwork = network([784,10,10])\n",
        "\n",
        "\n",
        "# creating the input layer\n",
        "activation = [np.zeros(x) for x in thisNetwork.layers]\n",
        "z = [np.zeros(x) for x in thisNetwork.layers]  # z is the matrix for the weighted inputs\n",
        "activation[0] = newdata[0]\n",
        "\n",
        "for x in range(0, thisNetwork.layerNum - 1):\n",
        "  for y in range(10):  #range(number of elements in thisNetwork.layers[x])\n",
        "    z[x+1][y] = np.dot(activation[x], thisNetwork.weights[x][y]) + thisNetwork.biases[x][y]\n",
        "    activation[x+1][y] = sigmoid(z[x+1][y])  #propagates forward the weighted input\n",
        "print(\"pre gradient descent: \", returnCost(activation[2], 0))\n",
        "\n",
        "#error\n",
        "error = [np.zeros(x) for x in thisNetwork.layers[1:]]\n",
        "error[1] = returnFinalError(activation[2], z[2], correct[0])\n",
        "error[0] = returnLayerError(error[1], z[1])\n",
        "  \n",
        "#grad descent\n",
        "updateBiases(error)\n",
        "updateWeights()\n",
        "print(\"post gradient descent: \", returnCost(activation[2], 0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2_SYqqIfScj",
        "outputId": "0a6d7155-f44e-44e9-b8db-f9bc2b67e32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pre gradient descent:  0.7730822667814958\n",
            "post gradient descent:  0.7730822667814958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tqEFB6vvwfSW"
      }
    }
  ]
}